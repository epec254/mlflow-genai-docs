---
sidebar_position: 5
pagination_next: tracing/index
---

import { Card, LogoCard, CardGroup, PageCard, SmallLogoCard } from "@site/src/components/Card";
import { APILink } from "@site/src/components/APILink";

# MLflow for GenAI

**MLflow enables AI developers to build production-grade GenAI applications that deliver business value. To deliver business value, GenAI applications must consistently provide high quality responses while optimizing for cost and latency.**

However, building GenAI applications presents unique challenges compared to both traditional software development and ML development.  Just as traditional software engineering relies on test suites and traditional ML relies on validation sets, GenAI applications need robust evaluation frameworks to ensure they deliver high-quality, performant, and cost-effective responses.  However, evaluating these aspects in GenAI is more complex:
- Unlike traditional software or ML, GenAI most often produces plain language outputs.  These outputs are either "right" or "wrong" but there are many right answers and some "right" answers are more correct than others.  Human feedback is often required to confidently assess these outputs.
- The non-deterministic nature of LLMs adds an extra layer of complexity - teams must evaluate not just for correctness, but for consistency and stability.

MLflow provides the infrastructure and tooling to enable AI developers to address these challenges by using an **evaluation-centric development workflow.**  An evaluation-centric workflow allows teams to:

- **Quickly experiment to improve quality/performance:** Iteratively refine prompts and model configurations, systematically comparing each iteration for quality, cost, and latency
- **Monitor production quality/performance in real-time:** Identify and quickly address failure modes that surface during production
- **Collaborate with business stakeholders:**  Define and measure quality standards based on input from key business stakeholders and domain experts
- **Define metrics to evaluate plain language outputs:** TALK ABOUT LLM JUDGES

## What is an evaluation-centric development workflow?

![Evaluation Driven Workflow](/images/genai/eval-driven-dev.png)

The most effective GenAI development workflows treat production as an extension of development. Teams deploy early versions to pre-production environments where beta testers can interact with the application. This traffic becomes valuable evaluation data, helping identify edge cases, failure modes, and performance bottlenecks that might not emerge in controlled testing. 

This evaluation-driven workflow continues in the production deployment. Teams monitor live traffic, identify new edge cases, failure modes, and performance issues, and use this real-world data to continuously update their evaluation data and improve their models and prompts. This ensures that GenAI applications maintain high quality, stay within cost constraints, and meet latency requirements as they evolve and encounter new scenarios.

Using this approach, teams can:
- Collect and analyze production traces to understand real-world usage patterns
- Identify problematic interactions and create targeted evaluation datasets
- Run offline evaluations on these datasets to test potential improvements
- Deploy refined versions and monitor their impact on key metrics

Without an evaluation-driven development workflow, teams risk deploying GenAI solutions that work well in demos but fail to deliver consistent value in production. By making evaluation a cornerstone of the development process and treating production as an extension of the development environment, organizations can more effectively bridge the gap between proof-of-concept and production-ready GenAI applications.


A developer is responsible for creating the following in code:
* Versions
* Scorers 

A developer is responsible for configuring
* What Alerts
* 


## Fundamental Elements of the Data Model

* Traces 
* Assessments
* Datasets
* Labeling Sessions
* Versions

## Core Development Workflows

## Core Production Workflows 


* Create and iterate on Scorers
* Test versions by running an evaluation 





* Schedule Scorers to run on Traces
* Send Traces for Human Labeling
* Curate Traces into Datasets
* Configure Alerts based on Assessments
* Identify quality and performance issues on Traces

<CardGroup>
  <PageCard
    headerText="Assessments"
    link="/getting-started/intro-quickstart/"
    text={[
      "Run MLflow server locally or use direct access mode (no server required) to run MLflow in your local environment. Click the card to learn more.",
    ]}
  />
  <LogoCard
    link="https://www.databricks.com/product/managed-mlflow"
    description="Databricks Managed MLflow is a FREE, fully managed solution, seamlessly integrated with Databricks ML/AI ecosystem, such as Unity Catalog, Model Serving, and more."
  >
    <span>![Databricks Logo](/images/logos/databricks-logo.png)</span>
  </LogoCard>
  <LogoCard
    link="https://aws.amazon.com/sagemaker-ai/experiments/"
    description="MLflow on Amazon SageMaker is a fully managed service for MLflow on AWS infrastructure,integrated with SageMaker's core capabilities such as Studio, Model Registry, and Inference."
  >
    <span>
      ![Amazon SageMaker Logo](/images/logos/amazon-sagemaker-logo.png)
    </span>
  </LogoCard>
  <LogoCard
    link="https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2"
    description="Azure Machine Learning workspaces are MLflow-compatible, allows you to use an Azure Machine Learning workspace the same way you use an MLflow server."
  >
    <span>![AzureML Logo](/images/logos/azure-ml-logo.png)</span>
  </LogoCard>
  <LogoCard
    link="https://nebius.com/services/managed-mlflow"
    description="Nebius, a cutting-edge cloud platform for GenAI explorers, offers a fully managed service for MLflow, streamlining LLM fine-tuning with MLflow's robust experiment tracking capabilities."
  >
    <span>![Nebius Logo](/images/logos/nebius-logo.png)</span>
  </LogoCard>
  <LogoCard
    link="https://mlflow.org/docs/latest/tracking.html#common-setups"
    description="You can use MLflow on your on-premise or cloud-managed Kubernetes cluster. Click this card to learn how to host MLflow on your own infrastructure."
  >
    <span>![Kubernetes Logo](/images/logos/kubernetes-logo.png)</span>
  </LogoCard>
</CardGroup>



* Logging versions

* Traces


Key differences
* Evaluations have the same set of inputs over time
* You can compare between evaluations

Evaluate existing traces

Data Model 

* Trace
  * Assessments
  * Trace Data 
  * Trace Info

* Dataset 

* Labeling Session 

* Evaluation Runs




Assessments
* feedback
* expectation




Scorers


Traces


Datasets


Monitor


Labeling









<details>
  <summary>Expand to See More about Key Challenges in LLM / GenAI Development</summary>

  #### Complexity in Compound AI Systems

  LLMs are often integral parts of larger, compound AI systems that incorporate multiple models, tools, prompts, and other components. With the rise of autonomous agents, the system's control flow becomes more dynamic and complex. Effectively managing such systems requires well-defined processes and specialized tools to handle their intricacies.

  <div className="center-div" style={{ width: "80%", padding: "20px" }}>
    ![GenAI Release Cadence](/images/llms/llama-index/llama_index_workflow_graph.png)
  </div>

  #### Rapidly Evolving Industry

  The GenAI industry is evolving at a fast pace, with new models, tools, and libraries emerging daily. Keeping track of versions and managing dependencies is crucial to maintaining the stability and reproducibility of AI systems.

  The following diagram illustrates the high release frequency of some popular GenAI libraries; LangChain and Openai, compared to those of classical ML and DL. Without keeping track of the versions and dependencies, it can be challenging to maintain the stability and reproducibility of AI systems.

  <div className="center-div" style={{ width: "80%", padding: "20px" }}>
    ![GenAI Release Cadence](/images/llms/genai-release-cadence.png)
  </div>

  #### Debugging Challenges

  Debugging LLM-based applications is particularly difficult due to the compound nature of these systems and the high level of abstraction in many frameworks. Gaining a clear understanding of how LLMs and their interacting components behave can be a daunting task without the right tools and techniques.

  #### Quality Assurance

  Ensuring the quality of machine learning systems has always been a challenge. With LLMs, the models' complexity and the dynamic nature of tasks make it even harder to assess and guarantee quality. Robust QA processes are essential to ensure reliability and performance.

  #### Production Monitoring

  As LLM-based applications become more dynamic, monitoring their performance post-deployment is increasingly critical to business success. Unlike traditional software monitoring, machine learning monitoring requires specialized tools to track model behavior and performance in real-time.

</details>

**MLflow's Support for LLMs** aims to alleviate these challenges by introducing a suite of features and tools designed with the end-user in mind.


## Why Choose MLflow?

- **ü™Ω Free and Open** - MLflow is **open source** and FREE. Avoid vendor lock-in and secure your ML assets on your own choice of infrastructure. You can invest in the most critical business goals without worrying about the cost of the MLOps platform.

- **üõ†Ô∏è Native Cloud Integration** - MLflow is not only self-hosting, but also offered managed service on Amazon SageMaker, Azure ML, Databricks, and more. This allows you to getting started quickly and scale easily within your existing cloud infrastructure.

- **ü§ù 15+ GenAI Framework Support** - MLflow integrates with more than a dozen GenAI libraries, including OpenAI, LangChain, LlamaIndex, DSPy, and more. This allows flexibility in choosing the right tool for your use case.

- **üîÑ End-to-End** - MLflow is designed for managing the end-to-end machine learning lifecycle, eliminating the complexity of managing multiple tools and moving assets between them.

- **üåç Domain Agnostic** - Real world problems are not always solved by GenAI alone. MLflow provides a unified platform for managing traditional ML, deep learning, and GenAI models, making it a versatile tool for all your ML needs.

- **üë• Community** - MLflow boasts a vibrant Open Source community as a part of the Linux Foundation. With **19,000+** GitHub Stars and **15MM+** monthly downloads, MLflow is a trusted standard in the MLOps/LLMOps ecosystem.


## How MLflow Solves LLMOps Challenges

MLflow offers a comprehensive suite of tools and features to simplify the development, deployment, and management of LLMs. Here are some of the key features that make MLflow the go-to platform for LLM development:

![LLM products](/images/llms/llm-products.png)

### Experiment Tracking

Experiment Tracking is a core fundamental feature of the MLflow platform, allowing you to organize many assets and artifacts during your LLM/GenAI projects, such as models, prompts, traces, metrics, in a single place. [‚û°Ô∏é Learn more](/tracking)

<details>
  <summary>Key Benefit of Experiment Tracking</summary>

  - **Single Source of Truth**: MLflow serves as a centralized location for storing and managing all your LLM assets, including models, prompts, and traces, ensuring that you have a single source of truth for your projects.

  - **Better Collaboration**: Experiment Tracking allows you to share your work with your team members, enabling them to reproduce your results and provide feedback.

  - **Lineage Tracking**: MLflow captures the lineage of your LLM assets throughout the different stages of the project, allowing you to trace the lineage of your models, prompts, and other artifacts.

  - **Comparative Analysis**: MLflow enables you to compare different versions of your LLM models, prompts, and other assets, helping you make informed decisions and improve the quality of your LLM applications.

  - **Governance and Compliance**: With the central repository of machine learning assets in your organization, MLflow helps you maintain governance and compliance standards, ensuring that your important assets are accessible only to authorized users.

</details>


<Card>
![Experiment Tracking](/images/llms/experiment-tracking.png)
</Card>



### Tracing / Observability

MLflow Tracing ensures LLM observability in your GenAI applications by capturing LLM calls and other important details, such as document retrieval, data queries and tool calls, allowing you to monitor and get deep insights into the inner workings of your applications. [‚û°Ô∏é Learn more](/tracing)

<details>
  <summary>Key Benefit of MLflow Tracing</summary>

  - **Debugging**: Traces provide a detailed view for what happens beneath the abstraction layer, helping you quickly identify and resolve issues in your LLM applications. With the native integration with **Jupyter Notebook**, MLflow offers a seamless debugging experience.

  - **Inspect Quality**: After evaluating your models or agents with [MLflow Evaluation](#evaluation), you can analyze the auto-generated traces to understand the behavior of your LLMs and make informed decisions.

  - **Production Monitoring**: Traces are essential for monitoring the performance of your LLMs in production. MLflow Tracing allows you to identify bottlenecks and performance issues in production, enabling you to take corrective actions and continuously optimize your applications.

  - **OpenTelemetry**: MLflow Traces are compatible with OpenTelemetry, an industry-standard observability framework, allowing you to integrate with popular observability tools like Prometheus, Grafana, and Jaeger for advanced monitoring and analysis.

  - **Framework Support**: MLflow Tracing supports 15+ GenAI libraries, including OpenAI, LangChain, LlamaIndex, DSPy, Anthropic, Amazon Bedrock, and more. With just adding **one line of code**, you can start tracing your LLM applications built with these libraries.


</details>

<Card >
![Tracing Gateway Video](/images/llms/tracing/tracing-top.gif)
</Card>


### Model Packaging

MLflow's model packaging empowers you to manage moving pieces of GenAI systems, including prompts, LLM parameters, fine-tuned weights, along with frozen dependency versions. This allows you to manage experiments with high **reproduceability**.

<details>
  <summary>Key Benefit of Model Packaging in MLflow</summary>

  - **Reproducibility**: MLflow ensures that all dependencies are captured and frozen during model packaging, ensuring that your models can be reproduced consistently across different environments.

  - **Versioning**: MLflow models are versioned and registered to the [MLflow Model Registry](/model-registry) before production deployment, allowing you to keep track of fast-evolving models and assets with ease.

  - **Native Support for Popular Packages**: Standardized interfaces for tasks like saving, logging, and managing inference configurations.

  - **Unified prediction APIs**: MLflow models can be loaded as a convenient `PyFunc` wrapper that exposes a consistent `predict()` API, regardless of the underlying library. This simplified the process of evaluation, deployment, and production use of GenAI systems.

  - **Model Signature**: MLflow's [model signature](/model/signatures) feature allows you to define the input, optional parameters, and output schema of your model and additional parameters, making it easier to understand how to interface with the model and share it with others.

</details>


MLflow provides native integrations for model packaging with major GenAI libraries. Is your favorite library not in the list below? No worries, you can package arbitrary model/agent using the MLflow's `PythonModel` framework. Click the icon below to learn more about each integration.

<CardGroup isSmall>
  <SmallLogoCard link="/llms/openai">
    <span>![OpenAI Logo](/images/logos/openai-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/llms/langchain">
    <span>![LangChain Logo](/images/logos/langchain-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/llms/llama-index">
    <span>![LlamaIndex Logo](/images/logos/llamaindex-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/llms/dspy">
    <span>![DSPy Logo](/images/logos/dspy-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/llms/transformers">
    <span>![HuggingFace Logo](/images/logos/huggingface-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/llms/sentence-transformers">
    <span>![SentenceTransformers Logo](/images/logos/sentence-transformers-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/model#promptflow-promptflow-experimental">
    <span>![PromptFlow Logo](/images/logos/promptflow-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="/llms/chat-model-intro">
    <b>Custom Packaging with PythonModel / ChatModel</b>
  </SmallLogoCard>
</CardGroup>


### Evaluation

MLflow's LLM Evaluation is designed to simplify the evaluation process, offering a streamlined approach to compare foundational models, prompts, and compound AI systems. [‚û°Ô∏é Learn more](/llms/llm-evaluate)


<details>
  <summary>Key Benefit of MLflow LLM Evaluation</summary>

  - **Simplified Evaluation**: Leverage MLflow's <APILink fn="mlflow.evaluate" /> API and built-in harnesses to test your GenAI models with ease.

  - **LLM-as-a-Judge**: MLflow provides built-in LLM-as-a-Judge metrics with flexible configuration for judge LLM providers, allowing you to plug-in complex evaluation criteria into automated evaluation process and to evaluate in bulk.

  - **Customizable Metrics**: Beyond the provided metrics, MLflow supports a plugin-style for custom scoring, enhancing the evaluation's flexibility.

  - **Comparative Analysis**: Effortlessly compare foundational models, providers, and prompts to make informed decisions.

  - **Deep Insights**: Traces are automatically generated for evaluation runs, allowing you to get deeper insights on the evaluation results and fix quality issues with confidence.


</details>


<Card >
![LLM Evaluate Video](/images/llms/llm-evaluate.gif)
</Card>

### Model Serving

MLflow models, once packaged, can be deployed easily to different targets including Kubernetes clusters, cloud providers (Amazon SageMaker, Azure Machine Learning), Databricks, and any serving infrastructure that is based on Docker containers. [‚û°Ô∏é Learn more](/deployment) 


<details>
  <summary>Key Benefit of MLflow Model Serving</summary>

  - **Effortless Deployment**: MLflow provides a simple interface for deploying models to various targets, eliminating the need to write boilerplate code.
  - **Dependency and Environment Management**: MLflow ensures that the deployment environment mirrors the training environment, capturing all dependencies. This guarantees that models run consistently, regardless of where they're deployed.
  - **Packaging Models and Code**: With MLflow, not just the model, but any supplementary code and configurations are packaged along with the deployment container. This ensures that the model can be executed seamlessly without any missing components.
  - **Avoid Vendor Lock-in**: MLflow provides a standard format for packaging models and unified APIs for deployment. You can easily switch between deployment targets without having to rewrite your code.
</details>

<Card>
  <div className="center-div" style={{ width: "90%" }}>
    ![](/images/deployment/mlflow-deployment-overview.png)
  </div>
</Card>

### Prompt Engineering UI

MLflow Prompt Engineering UI is a no-code playground for crafting and refining prompts for LLMs, integrated with MLflow's tracking and evaluation framework. [‚û°Ô∏é Learn more](/llms/prompt-engineering)

<details>
  <summary>Key Benefit of MLflow Prompt Engineering UI</summary>

  - **Iterative Development**: Streamlined process for trial and error without the overwhelming complexity.

  - **UI-Based Prototyping**: Prototype, iterate, and refine prompts without diving deep into code.

  - **Accessible Engineering**: Makes prompt engineering more user-friendly, speeding up experimentation.

  - **Optimized Configurations**: Quickly hone in on the best model configurations for tasks like question answering or document summarization.

  - **Transparent Tracking**: Every model iteration and configuration is meticulously tracked. Ensures reproducibility and transparency in your development process.

</details>

<Card>
![Prompt Engineering UI](/images/prompt_modal_2.png)
</Card>


### MLflow AI Gateway

Serving as a unified interface, MLflow AI Gateway simplifies interactions with multiple LLM providers and your own MLflow models. [‚û°Ô∏é Learn more](/llms/deployments) 

<details>
  <summary>Key Benefits of the MLflow AI Gateway</summary>

  - **Unified Endpoint**: No more juggling between multiple provider APIs.

  - **Simplified Integrations**: One-time setup, no repeated complex integrations.

  - **Secure Credential Management**: Centralized storage prevents scattered API keys. No hardcoding or user-handled keys.

  - **Consistent API Experience**: Uniform, easy-to-use REST API across all providers.

  - **Seamless Provider Swapping**: Swap providers without touching your code and zero downtime.

</details>


<CardGroup isSmall>
  <SmallLogoCard link="https://github.com/mlflow/mlflow/blob/master/examples/gateway/openai/README.md">
    <span>![OpenAI Logo](/images/logos/openai-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="https://github.com/mlflow/mlflow/blob/master/examples/gateway/openai/README.md">
    <span>![Anthropic Logo](/images/logos/anthropic-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="https://github.com/mlflow/mlflow/blob/master/examples/gateway/cohere/README.md">
    <span>![Cohere Logo](/images/logos/cohere-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="https://github.com/mlflow/mlflow/blob/master/examples/gateway/bedrock/README.md">
    <span style={{ minWidth: "80px" }}>![AWS BedLock Logo](/images/logos/bedrock-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="https://github.com/mlflow/mlflow/blob/master/examples/gateway/ai21_labs/README.md">
    <span>![ai21Labs Logo](/images/logos/ai21labs-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="https://github.com/mlflow/mlflow/blob/master/examples/gateway/azure_openai/README.md">
    <span>![Azure OpenAI Logo](/images/logos/azure-ml-logo.png)</span>
  </SmallLogoCard>
  <SmallLogoCard link="https://github.com/mlflow/mlflow/blob/master/examples/gateway/huggingface/README.md">
    <span>![Hugging Face Logo](/images/logos/huggingface-logo.svg)</span>
  </SmallLogoCard>
  <SmallLogoCard link="https://github.com/mlflow/mlflow/blob/master/examples/gateway/mosaicml/README.md">
    <span>![MosaicML Logo](/images/logos/mosaicml-logo.svg)</span>
  </SmallLogoCard>
</CardGroup>
<br />
